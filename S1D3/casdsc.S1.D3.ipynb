{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751a6395-1efc-405f-b124-58b34084e9bc",
   "metadata": {},
   "source": [
    "# CAS Big Data, Computational Methods, and Programming Summer Camp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702ac6f-cda1-4f08-ae12-7d3992cbe44a",
   "metadata": {},
   "source": [
    "### Day Three\n",
    "Today, we will be working with Python to collect data from websites using two different methods. The first is to use webscraping tools. These look at the actual code of the website and extract the things we ask it to. This is a great way to collect information from websites that you want to analyze or preserve but does require a bit of tinkering as well as some understanding of how HTML works. \n",
    "\n",
    "The second way we will use is through accessing an API. A good way to think about an API is to picture websites as an access point to a data storage unit. When you load a website, like The Washington Post, the stories and links that you see are pulled from a server where that data is saved. Social media functions in a similar way. The pictures you see on Instagram are not part of the code of the webpage, but are pulled from a server as you scroll.\n",
    "\n",
    "An API allows us to access the server directly without having to scrape a website. This is useful as many websites you might be interested in do not allow webscraping on their pages. However, it does require access credentials, can be very costly, and is limited to what the company will give you access to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872cb619-80e8-43cb-9baf-0330a6867c68",
   "metadata": {},
   "source": [
    "### Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09474e53-a384-4bb3-afa5-d1b4069638ac",
   "metadata": {},
   "source": [
    "To start webscraping, we will use requests and then beautfiulsoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19008b-b769-491f-b881-4c631167616f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276089d-5249-4bd8-bb10-d715926544b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df294f1a-6baf-40d7-bfeb-0fde3a34270e",
   "metadata": {},
   "source": [
    "Requests lets us collect the HTMl code from websites. First we will get the data from wikipedia using requests.get. Then we wil check the status code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d4ddf3-be58-4958-b1ae-c90932d811e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06e4af-3cc1-4831-ae96-ac6db2a50186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.govtrack.us/congress/members/current\")\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78cf3c0-c745-435e-85a7-6c80e32e53db",
   "metadata": {},
   "source": [
    "Some websites do not allow \"robots\" to access their sites. Look at the following example and see the status code we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af43f41-c039-43a1-9196-6e4d6f729b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.lansingcitypulse.com\")\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf999b-6770-44b7-b663-3ad062a9b588",
   "metadata": {},
   "source": [
    "Status codes:\n",
    "200 = OK\n",
    "\n",
    "Errors that start with 4 are likely errors in your code or something you are trying to do but not allowed to.\n",
    "\n",
    "- 400 = Bad request (your http is likely wrong)\n",
    "- 401 = Unauthorized (the website is locking you out and would do so on a browser)\n",
    "- 403 = Forbidden (website is blocking requests, you can try a header, described later)\n",
    "- 404 = Not Found (the website doesn't exist)\n",
    "- 408 = Request Timeout (website is taking too long to load)\n",
    "\n",
    "Errors that start with 5 are likely errors with either your internet connection or the website you are working with. Try and load the webpage using your browser)\n",
    "- 500 = Internal Server Error (problem with the website server)\n",
    "- 502 = Bad Gateway (problem with server)\n",
    "- 503 = Service Unavailable (problem with the server)\n",
    "\n",
    "You can find more errors here: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a9c32-db83-4a73-bfea-8d5d2f8a7822",
   "metadata": {},
   "source": [
    "Sometimes you can get by this by using headers, which tells the website you are a browser and not a script. However, this does not work on every website, especially those that are protected with Cloudflare. There are sometimes work arounds for Cloudflare, but they are complicated and we will not go over them in this camp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567d493-e634-40ad-a4c2-e79fceabd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.lansingcitypulse.com\")\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b82f80-1859-4e88-b7aa-ae5ad0cbd790",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'my-app/0.0.1'}\n",
    "r = requests.get('https://www.lansingcitypulse.com', headers=headers)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da35675-f4fc-46bd-802b-6c549ce1eaf0",
   "metadata": {},
   "source": [
    "#### Extracting Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c12512-7c1d-4a7a-af5c-370e3be8fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa471f-4526-4719-bd20-568a613d71b6",
   "metadata": {},
   "source": [
    "Requests allows us to get the HTML of a website, but we need to use a package called beautifulsoup to work with it. Here we will import beautifulsoup (called bs4) and look at the govtrack website for the members of the U.S. Congress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0fad4-3094-4b19-90bd-ff3039e8fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(bs4.BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af66fe-3fb5-4054-9fb7-cdbc731e84fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f64e6f-e34c-4d80-8dfa-9db070569ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942905f-971d-4afe-b2e2-34ae65364dca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a2260-c0f0-4110-96b6-5db7cf6fd1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296df122-3aaa-4f8b-8dc2-9afe25a9e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf19c6e-4877-4c46-9920-9b69eadafd6f",
   "metadata": {},
   "source": [
    "HTML Tags:\n",
    "- \\<HTML\\> Used at the start and end of a website\n",
    "- \\<head\\> Where the page resources are loaded, like the title, what is saved when you bookmark, etc.\n",
    "- \\<body\\> Where the site content actually is.\n",
    "- \\<div\\> Division tag, blocks of code for the site. Makes it easier to swap out content as the page owner goes, which means it is where the main content you want is saved.\n",
    "- \\<h1\\> Header tag, also h2, h3, etc.\n",
    "- \\<p\\> Paragraph tag, this is where your text content is saved at.\n",
    "\n",
    "There are a lot others as well, these are just the most common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89892d66-ce26-4c4b-95b3-d88c7c4f086d",
   "metadata": {},
   "source": [
    "#### An example using a fake HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084833f-1303-49c6-9193-b59a24f1c6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<p> This is a paragraph tag. </p>\n",
    "<p> This is another paragraph tag. </p>\n",
    "<a href=\"https://website.org\"> This is the text of the anchor tag</a>\n",
    "<a href=\"https://website2.org\"> More text for our anchor</a>\n",
    "<span class=\"blue\"> This is a span tag with the class blue </span>\n",
    "<span class=\"blue\"> This is another span tag with the class blue </span>\n",
    "<span class=\"red\"> This is a span tag with the class red </span>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced8330-c89f-4511-9e6b-5eb0b3b3e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "paras = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7339ffc-3496-4461-b3ee-efe1372ec641",
   "metadata": {},
   "outputs": [],
   "source": [
    "paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab059d-3a7d-4222-a354-8e17783eb281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in paras:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19561730-4076-4f8b-8966-af90a8a16d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d815666-c2e2-498c-8d85-f2fdff8354b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f26168-736f-4258-8bd2-0fce4b108e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "links[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8f75a-cd0a-47a5-bd4a-02f118df0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksText2 = []\n",
    "for i in links:\n",
    "    linksText2.append(i.get_text())\n",
    "linksText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3b226-07aa-46a0-9c4e-aa83080b2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linksText = [links.get_text() for links in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa962314-6f3c-4e24-ab73-702d0d103f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611d9f3-2867-483a-93a5-3dba63a7a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linksText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43adfdbb-5ea3-4db8-9640-8ddae2c0f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = soup.find_all('span', {'class' : 'blue'})\n",
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2c3ff-0ed7-4496-a623-deecd76de547",
   "metadata": {},
   "outputs": [],
   "source": [
    "spansText = [spans.get_text() for spans in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e554e77-4b20-483c-bee8-1e11682e5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795d569-a027-46a9-8636-393f5a556043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spansText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf6a82-a023-4bfb-8646-cf0b009d9386",
   "metadata": {},
   "source": [
    "#### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904bd32-7978-4ae7-9068-2877ca529482",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "<title> This is the title tag! </title>\n",
    "<td> This is a table tag</td>\n",
    "<li> li is for a list tag </li>\n",
    "<div id=\"id one\"> This is a divider or section tag with the id of one</div>\n",
    "<div id=\"id two\"> This is a divider or section tag with the id of one</div>\n",
    "<p> This is a paragraph tag. These are usually used for the actual text in the website.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf74e8-44ef-46ec-930b-48a23464652b",
   "metadata": {},
   "source": [
    "Run the cell above and then write some code to extract the title from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49b0d6-7b88-4a14-88d4-ce45027fc1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02a93bb6-9be8-44d1-a465-8cf20a8d3817",
   "metadata": {},
   "source": [
    "Now extract the table tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd957bac-f877-410e-a86c-8e61389f5794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0728499-3e3e-4908-9b0d-81514f4d2d8f",
   "metadata": {},
   "source": [
    "Now the list tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fbdb8b-0e93-41f9-95c5-ce98eedddbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6bc6589-a1a8-4564-90d5-a7380739ff48",
   "metadata": {},
   "source": [
    "Challenge: Extract the div tag with the id one. Here you will look for \"div\", id = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf993f0-8a8e-4af2-91b9-f9c86cf34f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f071132-c919-4ec8-8321-40b05d43c76e",
   "metadata": {},
   "source": [
    "#### Getting links or text\n",
    "The next set of codes looks at getting a list of links from a website then collecting the text those links are under. There are a variety of different ways to do this, this is one of them that works for the website in question. You will need to adapt other approaches for different websites if this doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc6aef-c215-41d1-a7e5-6be5aa46c1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "headers = {'user-agent': 'my-app/0.0.1'} #This is another way you can do headers.\n",
    "\n",
    "url = 'https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress'\n",
    "r = requests.get(url, headers = headers)\n",
    "soup = bs(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d7dc4c-99eb-4a09-a837-7c301e0e7363",
   "metadata": {},
   "source": [
    "Here we are going to use regular expressions to search for text that we care about. Regular expressions are like search terms that help you constrain what you are looking for. For instance, if you wanted to search for the words \"smile\", \"smiles\", and \"smiling\", you could write code that would look for all three of those. You could also search for \"smil\\S+\" where the \\S+ is a regular expression that will look any non-whitespace character, meaning it would find all three words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b99f53-b5c7-401a-9a36-0ca58b44849b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "links = soup.find_all('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f1903-93e6-4f35-8069-28b967595cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a', attrs={'href': re.compile(\"^https://\")})\n",
    "\n",
    "l = []\n",
    "for i in links:\n",
    "    l.append(i['href'])\n",
    "l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfcbf3-ba2d-438a-8e8d-7bd69f68f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe9f2c-913b-4082-aed5-922fabd35fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "links[45].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482f046-25e2-4a06-abd3-780adb4baa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a', attrs={'href': re.compile(\"^https://\")})\n",
    "\n",
    "l = []\n",
    "for i in links:\n",
    "    l.append(i.text)\n",
    "l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cddfa8-a2ec-471c-a80b-8838d9a38ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = []\n",
    "text = []\n",
    "\n",
    "links = soup.find_all('a', attrs={'href': re.compile(\"^https://\")})\n",
    "for i in links:\n",
    "    link.append(i['href'])\n",
    "    text.append(i.text)\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['Text'] = text\n",
    "df['URL'] = link\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbd25f-dbcc-40ba-9aa0-227ca975014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dup'] = df.duplicated(subset=['Text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e454f66-d120-4f50-a576-ba674698823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dup'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ccea5f-7b58-4158-af75-1ae41c298daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop_duplicates(subset=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10deafbe-e7a1-45f9-b5e5-fb61345d7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e7534-1071-49d2-8ea7-9ec482384b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc195e7f-1d21-4eb6-a33f-7ab31924df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = []\n",
    "text = []\n",
    "\n",
    "links = soup.find_all('a', attrs={'href': re.compile(\"^https://\")})\n",
    "for i in links:\n",
    "    link.append(i['href'])\n",
    "    text.append(i.text)\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['Text'] = text\n",
    "df['URL'] = link\n",
    "\n",
    "df['Text'] = df['Text'].replace('\\t','',regex = True)\n",
    "df['Text'] = df['Text'].replace('\\n','',regex = True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6b691-ddb5-4c7c-8935-3faa49f1753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dup'] = df.duplicated(subset=['Text'])\n",
    "df.head()\n",
    "df1 = df.drop_duplicates(subset=['Text'])\n",
    "print(df.shape)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944cd2a-30f9-4ce0-9fbf-cab23820e439",
   "metadata": {},
   "source": [
    "#### Another example using Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98096ce-f87b-4299-bf68-dcc65aad8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3193bd7-48f2-4a13-8550-1a69e5dd3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Mira_Potkonen\"\n",
    "\n",
    "r = requests.get(url)\n",
    "if r.status_code == requests.codes.ok:\n",
    "    print(r.text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cbae7f-d5fa-461d-8869-ee94cd3d0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = r.content\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6550e-e7cd-4bed-a993-ada80fa2fa62",
   "metadata": {},
   "source": [
    "This looks for the title tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e442982-2c2e-454d-a594-8e40aa4f8b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1195c11b-4444-42df-9f6a-f47b99ba9cab",
   "metadata": {},
   "source": [
    "Just the text of the title tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a64a4ec-d04e-4752-9570-ac1cda43d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcd244-44a9-4012-bec2-097763328246",
   "metadata": {},
   "source": [
    "First large header tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100a150-c313-4207-8a71-9c5154c33f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1eb97-ce7d-47c8-b2d4-1f401f49ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('h1').text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbfe74-e064-43be-bdec-1a59c9c18d4a",
   "metadata": {},
   "source": [
    "All anchor tags (links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba940fe2-ca8b-4f5e-9350-dac28c536824",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec3193-4285-44e4-a84a-3007f17a5b84",
   "metadata": {},
   "source": [
    "#### Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c49a06-22bb-430d-9773-8ff4cb6fc509",
   "metadata": {},
   "source": [
    "Write some code that selects the first table (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319ddd1-a22b-47df-af59-ce420defe54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b1a90d7-9fc5-4aa7-bc73-c75e0b396735",
   "metadata": {},
   "source": [
    "Write some code that selects all the items in lists on the page (li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e02ad7-67e0-4c66-9128-b204c94d9908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd27faa-2c02-48b5-9127-6944cda6773b",
   "metadata": {},
   "source": [
    "Write some code that selects the text within the first paragraph (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e9d44-b4a4-4640-8f42-fc59c39cfba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8996528-4733-4ebe-b790-e9b75b8c55b5",
   "metadata": {},
   "source": [
    "[Challenge] Use the Developer Tools in your browser to select some text on the website. Write code that will find that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d938e-8a95-40e2-bb20-12cd77463989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b19a1e56-9bd8-4efa-9292-4e43487e7015",
   "metadata": {},
   "source": [
    "#### Downloading multiple web pages\n",
    "- First, we'll grab all the URLs to medalist Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb52111-e05a-4f85-8525-e026e8f3abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Finland_at_the_Olympics\"\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d9122-b98b-4cbb-bc24-426e5cfa5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = r.content\n",
    "\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485d646-accb-4efc-bae3-6887464dd32d",
   "metadata": {},
   "source": [
    "We want to find the tables, specifically the one with the Summer Olympic medalists. It is the 9th table, but you would normally need to do a bit of trial and error to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27e3e5-02d5-4a4f-b261-7e24506ed272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tables = soup.findAll('table')\n",
    "tables\n",
    "table = tables[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005a69f-d8c4-4183-9950-834e3d183791",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00864c-7b9a-4b77-a41b-be895d9e7e6b",
   "metadata": {},
   "source": [
    "Now we are going to select the rows in the table and get the headers for the table, which is the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ec1e7-1b88-4531-9401-4b82436ae10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all('tr')\n",
    "rows[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7222c-448e-446c-bfff-19772b96930a",
   "metadata": {},
   "source": [
    "We can also look at the last row to find our friend Mira. From there, look for the second cell in the row. the td tag is for a cell in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b8da5-c2e7-46f0-9d98-4f16196ff88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = rows[-1].find_all('td')\n",
    "cells[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86765c0a-cc0d-43c3-938a-048887006f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9ae94-9d8b-4b1b-9575-e4a8e4d3c2a7",
   "metadata": {},
   "source": [
    "Now we can do something a bit more complicated. We are going to make an empty list and the loop through each row and find the links to each athelete's personal page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a93fb-fb6f-4959-bbcf-0e4e7586d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057582d-1368-484b-a636-b2870d06cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c01c1-901b-457f-aca1-085a83522301",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_athletes = []\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:\n",
    "        if (cells[1].find('a')):\n",
    "            link_to_athlete = cells[1].find('a')['href']\n",
    "            links_to_athletes.append(link_to_athlete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441dee0f-ceea-420b-a404-7c1f4c418fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4dd8a-8215-4446-9c5c-a26df3484772",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_athletes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ef013-5795-4b92-9b10-93e08a1f55a5",
   "metadata": {},
   "source": [
    "Now we can turn those strings into actual links by adding the first part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39c2e1-1efb-46ba-9a69-b0197b08ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_athletes = ['https://en.wikipedia.org' + i for i in links_to_athletes]\n",
    "links_to_athletes[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec8a43-c15a-42b9-9f9c-0b5173089096",
   "metadata": {},
   "source": [
    "Next, we'll visit each of the pages find the birthplace of each athlete and then add them to your list. We will use a line here to select the first HTML element span (span) that has the class attribute \"birthplace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ebf7d-d05b-424b-b549-fd767e9908a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for athlete_page in links_to_athletes:\n",
    "    r = requests.get(athlete_page)\n",
    "    page = r.content\n",
    "    \n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    birthplace = soup.find(\"span\", {\"class\": \"birthplace\"})\n",
    "    \n",
    "    if birthplace:\n",
    "        athlete_info = {}\n",
    "        athlete_info['name'] = soup.find('h1').text\n",
    "        athlete_info['birthplace'] = birthplace.text\n",
    "        data.append(athlete_info)\n",
    "    \n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66257c84-c661-4812-b10d-82736c4faed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17333a-8cbd-4578-881e-4d89f9c3ce68",
   "metadata": {},
   "source": [
    "### Check this data out in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb8c01-413a-4cf2-9590-9dd370c8bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc091f9-9d6d-4e91-a7f0-390e0248634c",
   "metadata": {},
   "source": [
    "We can select rows that contain athletes born in Helsinki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a04e5c-8b4d-4f1a-9c6a-f31110106124",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.birthplace==\"Helsinki, Finland\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d786b59-72e6-4d26-a561-71aa99314e9a",
   "metadata": {},
   "source": [
    "### Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97ec9d-84a8-4291-9cfa-8d246bd688c0",
   "metadata": {},
   "source": [
    "Select the row that contains our pal Mira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482777e-6a53-4d43-b020-8a31bced6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name==\"Paavo Nurmi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee9801-46c3-463a-82fb-f2b6df61771e",
   "metadata": {},
   "source": [
    "Use value_counts to see how many times each person appears in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f00eb5-ecd4-49e3-9dbc-b19b7d30935b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53ecc27b-cde1-425c-bfbd-2b3e8aa48860",
   "metadata": {},
   "source": [
    "#### Yet another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a6589-9c7d-4204-8564-d4bc67af429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "headers = {'user-agent': 'my-app/0.0.1'}\n",
    "\n",
    "url = 'https://time.com/6110450/kalamazoo-foundation-for-excellence/'\n",
    "r = requests.get(url, headers = headers)\n",
    "soup = bs(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b991a-53dd-43ed-b67c-055e5954e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cbe53-ddec-4173-8b1f-2c1ee7b60a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pList = soup.find_all('p')\n",
    "len(pList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ba7db-8dcd-4e27-990a-c1ec5f606502",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a642e2-702f-4218-a203-a37110abc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "myString = \"\"\n",
    "for i, para in enumerate(pList):\n",
    "    myString = myString + \"\\n\\n\" + para.text\n",
    "print(myString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2477caad-88c0-4db3-98c0-7ea41e20fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soupText = re.findall(\"residents\",soup.text)\n",
    "print(soupText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a094e-4b8b-462c-b616-932c478d61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soupText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a8fac-e9c3-4619-bf1f-f3ad0ea9b690",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920da41b-91bf-41ac-b569-dafebfd58f80",
   "metadata": {},
   "source": [
    "Go to Time Magazine's website and find an article you like. Then count how many paragraphs there are. We did this just above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2904ecf-58fb-449c-a1b3-c9b7f5a8ba81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ea8801-92e8-4a42-adb9-152f8dd36688",
   "metadata": {},
   "source": [
    "Save those paragraphs as an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67606bb-0783-4b6a-b2c3-a8924c5effc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dd193f4-b038-4389-b981-6ca4602e9af9",
   "metadata": {},
   "source": [
    "Look for a word that you might care about. For an extra challenge see if you can use a regular expression in your search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96424cd-73d5-45b7-b8a8-6a4e6456b328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edf9473e-4f0a-4c45-b7e3-936373266091",
   "metadata": {},
   "source": [
    "## APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e84a3-4bba-4e3c-899b-e5bcf68dfe93",
   "metadata": {},
   "source": [
    "As mentioned above, APIs allow us to access the database behind what we see on a website. For our example, we are going to use the Wikipedia API and the newsapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109b73e-5e7f-4b0e-8dd2-62abc13073ff",
   "metadata": {},
   "source": [
    "### Example using Wikipedia's API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae73d43-5aeb-48d6-b3da-44488ae573ba",
   "metadata": {},
   "source": [
    "There is also a package for Wikipedia's API. This does not require credentials, which is great for us. We do need to install the package first, then import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78add9-7749-4981-b6f1-4818585da443",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf017d67-de44-472a-aeb0-2dd1cf1e7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b53f34-467e-4074-a1ef-65484a240d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a unique user agent string\n",
    "# user_agent = 'MyWikipediaBot/1.0 (myemail@example.com)'\n",
    "\n",
    "# Initialize the Wikipedia object with the specified user agent\n",
    "# wiki_wiki = wikipediaapi.Wikipedia(\n",
    "#    language='en',\n",
    "#    user_agent=user_agent,\n",
    "#    extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfaef0b-9304-4afd-adc9-41f8286eeb15",
   "metadata": {},
   "source": [
    "We first need to set the language we want and then we can ask the API to give us a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3723991-7825-4306-b4b2-af7dd0873ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Wikipedia object\n",
    "wikiPage = wikipediaapi.Wikipedia(language='en')\n",
    "\n",
    "# Retrieve a Wikipedia page\n",
    "pagePy = wikiPage.page('Python_(programming_language)')\n",
    "\n",
    "# Display the first 400 characters of the page text\n",
    "print(pagePy.text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e266e1-19da-49dc-8009-733a968fa491",
   "metadata": {},
   "source": [
    "We can see what attributes are in our object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01dec4-8846-4a02-8557-7fd4615d7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(pagePy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c14bbd-ba5b-45ba-afc1-c891aa98248f",
   "metadata": {},
   "source": [
    "This will give us the title and summary of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa4000-e865-4011-8c88-775f516b7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagePy = wikiPage.page('Python_(programming_language)')\n",
    "\n",
    "print(\"Page - Title: %s\" % pagePy.title)\n",
    "print(\"Page - Summary: %s\" % pagePy.summary[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc7e1f-d69f-42a1-973c-0502389ad31f",
   "metadata": {},
   "source": [
    "All of the text of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c17ab-6d5d-4b58-b131-eafd1f0fe2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a Wikipedia page\n",
    "pWiki = wikiPage.page(\"Michigan State University\")\n",
    "\n",
    "# Display the first 400 characters of the page text\n",
    "print(pWiki.text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478dc53-ed21-420a-80e5-d54fbb29fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sections(sections, level=0):\n",
    "        for s in sections:\n",
    "                print(\"%s: %s - %s\" % (\"*\" * (level + 1), s.title, s.text[0:40]))\n",
    "                print_sections(s.sections, level + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a102b-f5ce-4023-84df-61d6bc1960c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sections(pWiki.sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8517a9-8f7c-493d-afdb-814dbba14851",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_history = pWiki.section_by_title('Campus')\n",
    "print(\"%s - %s\" % (section_history.title, section_history.text[0:140]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b863e-9ec0-413f-b03a-9d7c1c83faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pWiki.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b41c5-3e25-45e4-8cbe-ade551e79dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_links(page):\n",
    "    links = page.links\n",
    "    for title in sorted(links.keys()):\n",
    "        print(\"%s: %s\" % (title, links[title]))\n",
    "\n",
    "print_links(pWiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101342b-7437-46d9-867f-acdf0bf7e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the Wikipedia page for \"1920\"\n",
    "page_1920 = wiki_wiki.page('1920')\n",
    "\n",
    "# Get all sections by title \"January\"\n",
    "sections_january = page_1920.sections_by_title('January')\n",
    "\n",
    "# Display information about each section in \"January\"\n",
    "for s in sections_january:\n",
    "    print(f\"* {s.title} - {s.text[0:140]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a513549-9915-490a-aab6-617011da475b",
   "metadata": {},
   "source": [
    "As mentioned above, APIs allow us to access the database behind what we see on a website. For our example, we will be using NewsAPI. This API gives us access to news articles about topics we might care about. The basic version is free (though limited). It does require an API key, which you can get here: https://newsapi.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb4edb-387a-480f-8f8d-7d43ef875cc0",
   "metadata": {},
   "source": [
    "API calls are, at a basic level, a requests just like we did above. However, here we are building a URL with the data we want to get from the data server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554bdd66-59be-4b77-8904-c903d973c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ad436-21ba-4fbc-b381-a45d764fd708",
   "metadata": {},
   "source": [
    "The components of an API are the actual URL followed by a ? (often) and then the actual search you are executing plus your credential. There is variation here, but that's fairly standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f349de1-5660-4c96-aaae-35cb4d9d6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://newsapi.org/v2/top-headlines?country=us&apiKey=3b5f41ec5ac24aa2b053b934edded111\")\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90803bbf-c956-4ebc-af0e-0de97481974f",
   "metadata": {},
   "source": [
    "<h1> API Python Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59ae91-26e4-4b0a-b553-1cc23ffa92c5",
   "metadata": {},
   "source": [
    "While that works, it isn't always the best way to get API data. Instead, we can use a wrapper, which is just a Python package. We need to install the NewsAPI python wrapper. This is a shortcut to writing out requests but doesn't come preinstalled on Jupyter. The code to install the package is below. Once installed, we need to import a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202462a-25ba-46c3-b3bc-7b45956c4e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38288850-9a44-48ab-a08f-5f081d085cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "\n",
    "newsapi = NewsApiClient(api_key='3b5f41ec5ac24aa2b053b934edded111')\n",
    "\n",
    "headLines = newsapi.get_everything(q='michigan',\n",
    "                                          sources='abc-news',\n",
    "                                          language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488e740-f18a-4f13-b1a7-c4d226c8a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(headLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167b628-996b-45f4-9aa4-f5e9c5d470de",
   "metadata": {},
   "outputs": [],
   "source": [
    "headLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a12e4-8aa8-4950-bf24-16e9118240a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = newsapi.get_sources(country=\"us\")\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886341a3-aa23-48f0-be6c-46ef32902ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ee207-496b-4868-9494-3cb9df4b411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(sources)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674bab7b-58f3-4091-a682-6564da9486de",
   "metadata": {},
   "outputs": [],
   "source": [
    "headLines = newsapi.get_everything(q='michigan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4ddcc-bad6-4b3b-904c-4fcee3d0ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa55cb5-48f7-45c1-885d-69b40eee0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(headLines)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f632269-451a-4981-9ac7-f9feba02c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = headLines['articles']\n",
    "s = pd.DataFrame(articles)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df22408-050d-4dbc-aa87-3581b27b0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf89fb-9311-49c5-a7f5-975208935167",
   "metadata": {},
   "source": [
    "We can add more to our API call if we want. Retrieve news articles by keyword and from specific sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62bae9-01f9-493b-9947-dba298b01e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "headLines = newsapi.get_everything(q='michigan',\n",
    "                                   language='en',\n",
    "                                   from_param='2024-04-10',\n",
    "                                   sources ='associated-press,abc-news'                                   \n",
    "                                  )\n",
    "articles = headLines['articles']\n",
    "s = pd.DataFrame(articles)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23643c1e-9d93-410e-af65-a99ec40eafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f085fb1-7161-45e5-a78e-20edb21cc61a",
   "metadata": {},
   "source": [
    "List available news sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ecf38-5f76-4020-b7e0-ad95af263e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve news sources in the US\n",
    "sources_us = newsapi.get_sources(country='us')\n",
    "\n",
    "# Extract sources and convert to DataFrame\n",
    "df_sources_us = pd.DataFrame(sources_us['sources'])\n",
    "print(df_sources_us.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec979d-ef44-40e2-bc0e-c3cdd6442cdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Filter news by date range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd166a-4eff-4f65-94d3-94eb1a4cb17e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve articles about \"climate change\" from the past 30 days\n",
    "headlines_climate = newsapi.get_everything(\n",
    "    q='climate change',\n",
    "    from_param='2024-04-10',\n",
    "    language='en'\n",
    ")\n",
    "\n",
    "# Display the total number of results\n",
    "print(f\"Total Results: {headlines_climate['totalResults']}\")\n",
    "\n",
    "# Extract articles and convert to DataFrame\n",
    "articles_climate = headlines_climate['articles']\n",
    "df_climate = pd.DataFrame(articles_climate)\n",
    "print(df_climate.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d67c85-fbd2-4b3e-b683-d7acc00e408d",
   "metadata": {},
   "source": [
    "Pagination for Large Result Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fed31-2f84-4db2-9775-f85612e08e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the first page to estimate the total number of results\n",
    "headlines_soccer_first_page = newsapi.get_everything(q='soccer', language='en')\n",
    "total_results = headlines_soccer_first_page['totalResults']\n",
    "print(f\"Total Results: {total_results}\")\n",
    "\n",
    "# Calculate the number of pages (limited to 100 results per page)\n",
    "paginate = min(round(total_results / 100) + 1, 5)  # Limiting to 5 pages for the example\n",
    "\n",
    "# Retrieve articles across pages\n",
    "articles_soccer = []\n",
    "for i in range(1, paginate + 1):\n",
    "    headlines_soccer_page = newsapi.get_everything(q='soccer', language='en', page=i)\n",
    "    articles_soccer.extend(headlines_soccer_page['articles'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_soccer = pd.DataFrame(articles_soccer)\n",
    "print(df_soccer.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cfd0f4-2309-4074-a471-9d235a324c0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22c922-b1cf-44c3-9261-97cb1e29e4a0",
   "metadata": {},
   "source": [
    "Use the NewsAPI to collect some data. It can be anything you want. Print the headLines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e6340-b809-4b6b-b080-230f6c858ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba9f3e29-b65b-4d2e-9adf-c7ac111d2645",
   "metadata": {},
   "source": [
    "Now retrieve articles from \"BBC News\" only and display the first 5 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d331e-fa4f-4062-851e-7af7b7f58551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8dfb326-0c60-429b-8979-94fa6b9f19d9",
   "metadata": {},
   "source": [
    "Retrieve articles about \"climate change\" from the past 30 days and display the total results and the first 5 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a5c28-2432-40e2-9499-36221452b9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4b93fee-be94-4d31-851b-8d001af41a0d",
   "metadata": {},
   "source": [
    "Retrieve a list of all available news sources in the US, and display the first 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b7e1f-15fa-43ae-bc5d-0200bdf63cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15c9e288-4ade-45ec-a79d-39650a253bee",
   "metadata": {},
   "source": [
    "<h2>YouTube Data API: Allows for video searches, channel statistics, and playlist management.\n",
    "<h3> Python Wrapper: google-api-python-client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81dd7ed-9485-4eaa-9fee-4413ca6473be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3c14a-09e7-4c7b-97c1-c861c5ab3839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Initialize the YouTube client with your API key\n",
    "youtube = build('youtube', 'v3', developerKey='AIzaSyDmIw5P9uvMAGHTyP68CLr_PHl6QIXp2MA')\n",
    "\n",
    "# Search for videos related to \"machine learning\"\n",
    "request = youtube.search().list(\n",
    "    q='machine learning',\n",
    "    part='snippet',\n",
    "    type='video',\n",
    "    maxResults=5\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "# Display the video titles and IDs\n",
    "for item in response['items']:\n",
    "    print(f\"Video Title: {item['snippet']['title']}\")\n",
    "    print(f\"Video ID: {item['id']['videoId']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309bccde-d462-4ec6-aae9-64cc9f6d2d9c",
   "metadata": {},
   "source": [
    "<h3> Retrieve all videos in a specified playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d855428-4e5b-4384-970e-cca4c5f9c356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Initialize the YouTube Data API client\n",
    "youtube = build('youtube', 'v3', developerKey='AIzaSyDmIw5P9uvMAGHTyP68CLr_PHl6QIXp2MA')\n",
    "\n",
    "# Set the playlist ID\n",
    "playlist_id = 'PLINj2JJM1jxNOvEFIABOBa6OmURYOxOk3'\n",
    "\n",
    "# Get all videos in the specified playlist\n",
    "request = youtube.playlistItems().list(\n",
    "    playlistId=playlist_id,\n",
    "    part='snippet',\n",
    "    maxResults=5  # Adjust the number of results per page as needed\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "# Display the video titles and IDs\n",
    "for item in response['items']:\n",
    "    print(f\"Video Title: {item['snippet']['title']}\")\n",
    "    print(f\"Video ID: {item['snippet']['resourceId']['videoId']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239303f8-9c9a-401b-b107-c66686bede87",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3> Retrieve channel information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c9001-28d4-4a98-b6e7-820ec70405b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the channel username or ID \n",
    "channel_username = 'TaylorSwift'\n",
    "\n",
    "# Get the channel information\n",
    "request = youtube.channels().list(\n",
    "    forUsername=channel_username,\n",
    "    part='snippet,statistics'\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "# Display the channel information\n",
    "for item in response['items']:\n",
    "    print(f\"Channel Title: {item['snippet']['title']}\")\n",
    "    print(f\"Subscribers: {item['statistics']['subscriberCount']}\")\n",
    "    print(f\"Total Views: {item['statistics']['viewCount']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b6b56-4dac-4c50-8e05-41c83232bfac",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3> Retrieve Trending Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0bae73-fc7f-4432-aa40-38e9a0feeb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get trending videos in the US\n",
    "request = youtube.videos().list(\n",
    "    chart='mostPopular',\n",
    "    regionCode='US',\n",
    "    part='snippet,statistics',\n",
    "    maxResults=5\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "# Display the video titles and statistics\n",
    "for item in response['items']:\n",
    "    print(f\"Video Title: {item['snippet']['title']}\")\n",
    "    print(f\"Views: {item['statistics']['viewCount']}\")\n",
    "    print(f\"Likes: {item['statistics']['likeCount']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960e8ba-a013-46be-bc7c-1e5a2e9df734",
   "metadata": {},
   "source": [
    "<H3> Retrieve Channel's Playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3d638-9c0d-4f60-8f31-ebd0b8973753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the channel ID \n",
    "channel_id = 'UCqECaJ8Gagnn7YCbPEzWH6g'\n",
    "\n",
    "# Get all playlists for the specified channel\n",
    "request = youtube.playlists().list(\n",
    "    channelId=channel_id,\n",
    "    part='snippet',\n",
    "    maxResults=5\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "# Display the playlist titles and IDs\n",
    "for item in response['items']:\n",
    "    print(f\"Playlist Title: {item['snippet']['title']}\")\n",
    "    print(f\"Playlist ID: {item['id']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b9e71-4a6c-47e2-90a1-e81334c22b4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3> Retrieve Video Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f2d62-03e3-486c-a056-78e3504be6aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the video ID \n",
    "video_id = 'nfWlot6h_JM'\n",
    "\n",
    "# Get comments for the specified video\n",
    "request = youtube.commentThreads().list(\n",
    "    videoId=video_id,\n",
    "    part='snippet',\n",
    "    maxResults=5\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "# Display the comments\n",
    "for item in response['items']:\n",
    "    comment = item['snippet']['topLevelComment']['snippet']\n",
    "    print(f\"Author: {comment['authorDisplayName']}\")\n",
    "    print(f\"Comment: {comment['textOriginal']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe98f99-9c00-4bd2-aa81-dfd2f86633e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
